{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are those cells doing?\n",
    "\n",
    "### This lesson will help you analyze a dataset from the Allen Brain Institute's Brain Observatory.\n",
    "\n",
    "These datasets contain calcium imaging data for various different cell types in the visual cortex of the mouse. It's likely that these cell types have different roles in the visual system -- your mission is to figure out what these roles are. You will choose a visual area, a cell type, and look at their responses to natural stimuli:\n",
    "\n",
    "<img src=\"ExperimentalDesign.png\">  \n",
    "\n",
    "### By the end of this lesson, you will be able to:\n",
    "1. Choose a dataset for a particular cell type, in a specific visual area.\n",
    "2. Plot a stimulus-response curve for one neuron.\n",
    "\n",
    "Additional information on this dataset, and how it was collected, can be found <a href=\"http://help.brain-map.org/display/observatory/Data+-+Visual+Coding\">here</a> as well as in the <a href=\"https://media.readthedocs.org/pdf/allensdk/latest/allensdk.pdf\">SDK documentation</a>. \n",
    "\n",
    "\n",
    "## Step 1. Importing toolboxes\n",
    "<b>First, we'll import the necessary toolboxes to run this code.</b> The first chunk of \"import\" lines will bring in some standard toolboxes that we need. For example, \"numpy\" is a toolbox that has functions to work with large arrays (https://en.wikipedia.org/wiki/NumPy). The second chunk of import lines brings in some toolboxes that the Allen Brain Observatory has already packaged, to help users analyze its data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard toolboxes\n",
    "import pprint\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Allen specific toolboxes\n",
    "import allensdk.brain_observatory.stimulus_info as stim_info\n",
    "from allensdk.core.brain_observatory_cache import BrainObservatoryCache\n",
    "from allensdk.brain_observatory.natural_scenes import NaturalScenes\n",
    "from allensdk.brain_observatory.drifting_gratings import DriftingGratings\n",
    "from allensdk.brain_observatory.static_gratings import StaticGratings\n",
    "\n",
    "# We will save the Brain Observatory Cache as a variable, \"boc.\"\n",
    "boc = BrainObservatoryCache(manifest_file='/datasets/allen-brain-observatory/visual-coding-2p/manifest.json')\n",
    "cwd = os.getcwd()\n",
    "print('Successfully imported AllenSDK packages.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Get a list of all possible trangenic mouse lines and brain areas, and choose which to work with.\n",
    "Next, we'll ask that \"boc\" structure to tell us what all of the possible Cre lines and brain areas are that we can analyze. You'll need to use these exact names when you're trying to pull a specific one from the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll save the list of cre lines as a variable, 'cre-lines'.\n",
    "cre_lines = boc.get_all_cre_lines()\n",
    "print(\"all cre lines: \" + str(cre_lines))\n",
    "\n",
    "# We'll save the list of possible structures as a variable, 'brain_areas'.\n",
    "brain_areas = boc.get_all_targeted_structures()\n",
    "print(\"all brain regions: \" + str(brain_areas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p><b>Task:</b>  Choose a visual area and Cre line from the lists above to examine in the rest of the notebook. You can find more info about the Cre-lines here (you'll need this for your write-up) <a href=\"http://observatory.brain-map.org/visualcoding/transgenic\">here</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the visual area & cre line below in single quotes. e.g. 'VISp'\n",
    "visual_area = \n",
    "cre_line = \n",
    "print(\"Let's take a look at how \" + str(cre_line)\n",
    "      + \" cells in \" + visual_area + \" respond to natural scenes. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next bit of code will get the list of all the experiment containers for that area and Cre line combination, and make a data frame so we can see what information we have on these experiments. We can use that table to confirm we've got the data we wanted (from the correct Cre line and structure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exps = boc.get_experiment_containers(\n",
    "    targeted_structures=[visual_area],\n",
    "    cre_lines=[cre_line],)\n",
    "\n",
    "#create a dataframe and show us the beginning of it\n",
    "exps_df = pd.DataFrame(exps)\n",
    "exps_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p><b>Task:</b>  Pick an experiment from the table above. Be sure to use the \"id\" column.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_container_id = \n",
    "print(experiment_container_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll get information about all of the experiment <strong>sessions</strong> in your experiment <strong>container</strong>.  This is accomplished with the `get_ophys_experiments` method. Most experiments have three different sessions, for different types of visual stimuli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "expt_cont = boc.get_ophys_experiments(\n",
    "    experiment_container_ids=[experiment_container_id], \n",
    "    stimuli=['natural_scenes'],\n",
    ")\n",
    "session_id = expt_cont[0]['id']\n",
    "data_set = boc.get_ophys_experiment_data(ophys_experiment_id=session_id)\n",
    "print('Data acquired for session '+str(session_id))\n",
    "pd.DataFrame(expt_cont)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Download & inspect the natural scenes imaging session\n",
    "First, we'll look at the session where the mouse viewed natural scenes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at the data you just acquired. We'll take a maximum projection of the data, so that we can actually see the cells. If we just looked at one snapshot, the cells would look dim. A maximum projection shows us the maximum brightness for each pixel, across the entire experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_projection = data_set.get_max_projection()\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "plt.imshow(max_projection, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see our cells in action! First, we need to prepare a few things..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import animation, rc\n",
    "from IPython.display import HTML\n",
    "\n",
    "raw_data_dir = '/datasets/allen-brain-observatory/visual-coding-2p/ophys_movies/'\n",
    "def get_raw_data_path(session_id):\n",
    "    return os.path.join(raw_data_dir, 'ophys_experiment_'+str(session_id)+'.h5')\n",
    "\n",
    "exp_path = get_raw_data_path(session_id)\n",
    "raw_data = h5py.File(exp_path, 'r')\n",
    "raw_data['data']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "im = ax.imshow(raw_data['data'][0])\n",
    "ax.axis('off')\n",
    "\n",
    "def init():\n",
    "    im.set_data(raw_data['data'][0])\n",
    "    return (im,)\n",
    "\n",
    "def animate(i):\n",
    "    im.set_data(raw_data['data'][i])\n",
    "    return (im,)\n",
    "\n",
    "anim = animation.FuncAnimation(fig, animate, init_func=init, frames=30, interval=1000./30, blit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can actually play the animation. Run the code below & press play. <b>Note</b> this might take a few seconds to load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Look at the calcium transients of your cells\n",
    "\n",
    "Now we'll plot the data of each of our cells (from the field of view above) across time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ts, dff = data_set.get_dff_traces()\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "for i in range(50):\n",
    "    plt.plot(dff[i]+(i*2), color='gray')\n",
    "    plt.xlabel('frames')\n",
    "    plt.ylabel('cell #')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p><b>Task:</b> Choose one cell to look at. The plot above gives you an idea of how many cells there are.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose a number somewhere between 0 and the # cells above\n",
    "my_cell = \n",
    "cell_specimen_ids = data_set.get_cell_specimen_ids()\n",
    "cell_id = cell_specimen_ids[1]\n",
    "\n",
    "# get raw traces for that cell\n",
    "time, raw_traces = data_set.get_fluorescence_traces(cell_specimen_ids=[cell_id])\n",
    "\n",
    "# plot raw and corrected ROI trace\n",
    "plt.figure(figsize=(14,4))\n",
    "plt.plot(time[:len(dff[0])], dff[0])\n",
    "plt.xlabel('frames')\n",
    "plt.ylabel('deltaF/F')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Look at the response of your cell to natural scenes\n",
    "Hmm, there are some responses above, but it's tough to see what's going on with just the raw traces. Let's see how these cells actually responded to different types of images.\n",
    "\n",
    "First, we'll organize the stimulus table. This tells us which stimulus was played on each trial. This data set has 118 different scenes, and each scene is presented 50 times. Images of the scenes can be found here: http://observatory.brain-map.org/visualcoding/stimulus/natural_scenes\n",
    "\n",
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p><b>Task:</b> Choose one of the natural scenes (there are 118 total). Put next to \"image_id\" below. Try another one if you're not happy with your first random choice.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#choose your image id\n",
    "image_id = \n",
    "\n",
    "natural_scene_table = data_set.get_stimulus_table('natural_scenes')\n",
    "natural_scene_template = data_set.get_stimulus_template('natural_scenes')\n",
    "sceneIDs = np.unique(natural_scene_table.frame)\n",
    "\n",
    "# plot this natural scene\n",
    "plt.imshow(natural_scene_template[image_id,:,:], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll find all of the cells that significantly prefer this stimulus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the experiment_container id for this session\n",
    "this_expt = expt_cont['id'==session_id]\n",
    "experiment_container_id = this_expt['experiment_container_id']\n",
    "cell_specimens = pd.DataFrame(boc.get_cell_specimens())\n",
    "subset = cell_specimens[cell_specimens.experiment_container_id==experiment_container_id]\n",
    "subset[(subset.p_ns<0.05)&(subset.pref_image_ns==image_id)].cell_specimen_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the data of single trials where a single cell was shown your images above. If the above doesn't give you any results, change the image above and run it again.\n",
    "\n",
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "<p><b>Task:</b> Choose one of the cell specimen ids from above and enter it below. The cell specimen ids are the longer numbers.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cell_id = \n",
    "ts, dff = data_set.get_dff_traces()\n",
    "cell_index = data_set.get_cell_specimen_indices([cell_id])[0]\n",
    "\n",
    "stim_subset = natural_scene_table[natural_scene_table.frame==image_id]\n",
    "for i in range(len(stim_subset)):\n",
    "    plt.plot(dff[cell_index,stim_subset.start.iloc[i]-10:stim_subset.end.iloc[i]+10], color='gray')\n",
    "plt.axvspan(10,18, color='red',alpha=0.2)\n",
    "plt.ylabel('deltaF/F')\n",
    "plt.xlabel('frames')\n",
    "plt.title('Single trial responses to selected natural scene')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, a few cells in your dataset like this particular image, but what about all of the cells? And what about all of the other images? To visualize this, we'll plot a histogram of how many cells significantly prefer certain images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "axes = subset[(subset.p_ns<0.05)].pref_image_ns.plot(kind=\"hist\",fc=(0, .5,.5, 0.5),bins=117)\n",
    "axes.set_ylabel('# cells that preferred image')\n",
    "axes.set_xlabel('image id')\n",
    "plt.show()\n",
    "\n",
    "n, bins, patches = ax.hist(subset[(subset.p_ns<0.05)].pref_image_ns, 117, density=1)\n",
    "\n",
    "print('Top image is '+str(n.argmax()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which images do the cells in your experiment actually respond to? Scroll back up to the block where you plotted your first preferred image and change the image ID so that you can see what the image actually is.\n",
    "\n",
    "<b>Save the histogram above for your lab write-up.<b/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6. Examine the direction selectivity of your cell\n",
    "Sometimes, the function of a cell is not particularly clear from natural stimuli. Those stimuli have a lot of information in them, and it might be hard to tell what a cell is actually responding to. Instead, we can use simple drifting gratings to look at one straightforward property of a cell: <b>does it respond to specific directions of movement?</b></br>\n",
    "\n",
    "We'll take the same cell you looked at above, but look at its response to drifting gratings. Below, we'll plot the response of this cell to different directions of the grating.\n",
    "\n",
    "First, let's look at the raw data in response to a single stimulus. The code below will plot several seconds of data during a drifting grating. The stimulus was played while the background of the plot is gray. If there isn't a clear response during the stimulus, change the number in \"stim_table.orientation\" until you find one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the stim_id below to find a good response\n",
    "stim_id = 0\n",
    "exps = boc.get_ophys_experiments(cell_specimen_ids=[cell_id],\n",
    "                stimuli=['drifting_gratings'],targeted_structures=[visual_area],\n",
    "                cre_lines=[cre_line])\n",
    "session_id = exps[0]['id']\n",
    "data_set = boc.get_ophys_experiment_data(session_id)\n",
    "timestamps, dff = data_set.get_dff_traces(cell_specimen_ids=[cell_id])\n",
    "dff_trace = dff[0,:]\n",
    "stim_table = data_set.get_stimulus_table('drifting_gratings')\n",
    "\n",
    "plt.plot(dff_trace[stim_table.start[stim_id]-30:stim_table.end[stim_id]+30])\n",
    "plt.axvspan(30,90, color='gray', alpha=0.3) #this shades the period when the stimulus is being presented\n",
    "plt.ylabel(\"DF/F\")\n",
    "plt.xlabel(\"Frames\")\n",
    "plt.title('response to '+ str(stim_table.orientation[stim_id])+' degrees')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just the response to one stimulus -- it might be quite noisy (or you might have found a good one!). If there isn't a clear response (if the trace doesn't rise when the stimulus comes on) change the stim_id in the code above and re-run. Once you've found a clear response, <b>save the image for your lab write-up</b>.\n",
    "\n",
    "Next, let's look across all directions of the stimulus, to see if we can find any patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_response= np.zeros((len(stim_table),3))\n",
    "for i in range(len(stim_table)):\n",
    "    cell_response[i,0] = stim_table.orientation[i]\n",
    "    cell_response[i,1] = stim_table.temporal_frequency[i]\n",
    "    cell_response[i,2] = dff_trace[stim_table.start[i]:stim_table.end[i]].mean()\n",
    "    \n",
    "all_ori = np.unique(cell_response[:,0])\n",
    "orivals = all_ori[np.isfinite(all_ori)]\n",
    "np.sort(stim_table.orientation.dropna().unique())\n",
    "tfvals = np.unique(cell_response[:,1])\n",
    "tfvals = tfvals[np.isfinite(tfvals)]\n",
    "tuning_array = np.empty((8,5))\n",
    "for i,ori in enumerate(orivals):\n",
    "    for j,tf in enumerate(tfvals):\n",
    "        trials = np.where(np.logical_and(cell_response[:,0]==ori, cell_response[:,1]==tf))[0]\n",
    "        tuning_array[i,j] = cell_response[trials,2].mean()\n",
    "for i in range(5):\n",
    "    plt.plot(orivals, tuning_array[:,i], 'o-')\n",
    "    plt.xlabel('direction of stimulus')\n",
    "    plt.ylabel('DF/F')\n",
    "    plt.title('Direction selectivity')\n",
    "\n",
    "cell_data = subset[subset.cell_specimen_id==[cell_id]]\n",
    "cell_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is your cell direction selective? In other words, does it clearly respond to one orientation over other orientations? It may also be orientation selective -- responding to two, opposite directions. Take a look at <b>dsi_dg</b> (direction selectivity index) in the table above to see whether or not your cell DSI is above 0.5. Typically, we consider 0.5 or above to mean the cell is direction selective.\n",
    "\n",
    "<b>Save this plot for your Canvas write-up.</b>\n",
    "\n",
    "That's all folks. Submit your write-up on Canvas to get your points for the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML('<img src=\"https://i.chzbgr.com/full/5107765760/h244F968D/\">')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
